%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2016 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2016,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure}

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% Maths
\usepackage{amsmath}
\usepackage{amssymb}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2016} with
% \usepackage[nohyperref]{icml2016} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2016}

% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2016}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Deep Manifold Learning on Music Audio for Navigating Large Sound Libraries}

\begin{document}

\twocolumn[
\icmltitle{Deep Manifold Learning on Music Audio \\
           for Navigating Large Sound Libraries}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2016
% package.
\icmlauthor{Eric J. Humphrey}{ejhumphrey@spotify.com}
\icmladdress{Spotify USA,
            620 Avenue of the Americas, New York City, NY 10011 USA}
% \icmlauthor{Your CoAuthor's Name}{email@coauthordomain.edu}
% \icmladdress{Their Fantastic Institute,
%             27182 Exp St., Toronto, ON M6H 2T1 CANADA}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{boring formatting information, machine learning, ICML}

\vskip 0.3in
]

\begin{abstract}

Use data to learn low-dimensional representations of acoustic similarity for visualization and browsing, discover latent manifolds in the data.
We present an approach leveraging different neighborhood relationships drawn between a large collection of instrument samples.
Describe insights and produce a number of examples that illustrate the behaviors induced by this approach.
Describe insights and produce a number of examples that illustrate the behaviors induced by this approach.


\end{abstract}

\section{Introduction}
\label{submission}

% A query for ``voice'' returns over 19k results on FreeSound, a collaborative database of audio clips.

Navigating large audio sample libraries has long been a pain point for musicians and sound artists alike.
Search queries are predominantly forced to take the form of text, which is problematic for at least two reasons.
Metaphors and descriptive tags, when provided, struggle to capture important characteristics in sufficient detail, and this language often varies from one individual to the next.
Alternatively, standard approaches to sound visualization -- waveform envelopes or spectrograms -- are hardly intuitive for the general population.
% Furthermore, such descriptions are not always associated with every sound in a collection, and typically only at the granularity of the entire recording.
% As a result, the task of navigating a sound library is often reduced to that of an exhaustive, brute force search.
As a result, the development of computational systems for acoustic similarity, an elusive concept in its own right, remains an open research topic.

% The link between ^ and v isn't stellar.
Acoustic similarity is a natural use case for manifold learning, which attempts to preserve relationships in some low dimensional space, typically for visualization.
Common embedding methods, such as Multidimensional Scaling (MDS) \cite{}, Locally Linear Embedding (LLE) \cite{}, or Isomap \cite{}, respect pairwise distances between observations, but exhibit two practical drawbacks:
these methods do not yield general functions that can be applied to new data, and obtaining accurate pairwise distances for large datasets is not scalable.
Ranking methods, like WSABIE \cite{weston2011wsabie}, relax this constraint in favor of maintaining relative order between observations, but these nuanced relationships can still be hard to obtain at scale.
Neighborhood methods, such as neighborhood components analysis (NCA) \cite{hinton2004neighborhood}, DrLIM \cite{hadsell2006drlim}, or \texttt{word2vec} \cite{mikolov2013distributed}, simplifiy the problem even further by exploiting unordered set relationships. %\footnote{Both NCA and word2vec can be understood as neighborhood methods, with probabilistic and context-driven interpretations.}
Rather than placing the burden of continuity on the data, neighborhood methods task the model with smoothly interpolating discrete sets in the embedding space.
 % easier to obtain, especially from large datasets, or be defined by coarse \emph{a priori} knowledge, \emph{e.g.} descriptive tags.
Prior work has demonstrated the potential for such methods to yield intutitive representations, where algebraic operations on vectors encode physical orientation \cite{hadsell2006drlim} or analogy \cite{mikolov2013efficient}.

Synthesizing these two topics, we explore the potential of neighborhood-based ``deep'' manifold learning with a large collection of instrument sounds for developing intuitive acoustic representations.
% Different kinds of neighborhoods are easy to draw from data, and we are curious to examine the different mappings they produce.
Deep neural networks provide a general framework for learning functions that map high-dimensional data into useful embeddings, which we influence by considering different kinds of relationships between inputs.
% Feature learning is particularly important for acoustic similarity, as the perceptual quality of ``timbre'' remains an ill-defined concept.
The behavior of the resulting embeddings is evaluated through both quantitative and qualitative means, yielding a variety of insights and audio-visual examples.

 % to identify any projections might prove useful in making sense of large sound collections.
% , this work explores the effects of leveraging different neighborhood relationships
% Therefore we are interested primarily in how these partitions are defined and sampled during training.



\section{Method}

We approach acoustic similarity by optimizing the parameters of a deep neural network to maximally preserve $K$ contrasting set relationships, \emph{i.e.} neighborhoods, between samples in a low-dimensional Euclidean space.
Given a collection of observations, $\mathcal{D}$, the $k^{th}$ contrastive parition consists of a positive, $\Gamma_k$, and a negative, $\bar{\Gamma_k}$, subset, satisfying three conditions:
one, contrastive partitions are internally disjoint, $\Gamma_k \cap \bar{\Gamma_k} = \varnothing$;
two, contrastive partitions may comprise a subset of the entire collection, $|\Gamma_k \cup \bar{\Gamma_k}| \le |\mathcal{D}|$;
and three, contrastive partitions are drawn independently of each other, such that any two partitions, $i$ and $j$, may share observations, $|\Gamma_i \cap \Gamma_j| \ge 0$.
Thus, the network can be understood as interpolating the various discrete partitions, with the training objective finding a smooth compromise between them.


\subsection{Model}

Audio is first transformed to a constant-Q representation, parameterized as follows:
signals are downsampled to 16kHz;
bins are spaced at 24 per octave, or quarter-tone resolution, and span eight octaves, from 27.5Hz to 7040Hz;
analysis is performed at a framerate of 20Hz uniformly across all frequency bins.
Logarithmic compression is applied to the frequency coefficients with an offset of one, i.e. $log(C*X + 1.0)$, where $C=50$.

Keeping with related work \cite{humphrey2015dl4mir}, windows of time-frequency coefficients are transformed by a five-layer convolutional neural network (CNN) into a 3-dimensional embedding for the purposes of visualization.
The network consists of three 3D-convolutional layers and two fully connected layers, with max-pooling by a factor of 2 in time for first two layers.
The first four layers use hyperbolic tangent activation functions, while the last layer is linear to avoid saturation.


\subsection{Learning a Mapping into Euclidean Space}

% Loss architecture
A contrastive loss function is used to optimize the model's parameters, following prior research on deep manifold learning \cite{hadsell2006drlim, humphrey2011nlse, humphrey2015dl4mir}.
However, we augment training criterion to use a ternary network configuration, rather than the pairwise one employed previously, defined as follows:

% loss_sim = hwr(cost_sim - margin_sim)^2
% loss_diff = hwr(margin_diff - cost_diff)^2
% total = ave(loss_sim + loss_diff)
% \vspace{-0.15in}
\begin{align*}
Z_i = \mathcal{F}(X_i | \Theta),~Z_p = \mathcal{F}(X_p | \Theta),~Z_n = \mathcal{F}(X_n | \Theta)\\
D_p = || Z_i - Z_p ||_2,~D_n = || Z_i - Z_n ||_2\\
\mathcal{L} = \max(0, D_p^2 - m_{p}) + \max(0, m_{n} - D_n)^2 \\
\end{align*}
% \vspace{-0.15in}

Here, an observation, $X_i$, a positive neighbor, $X_p$, and a negative example $X_n$, are transformed by the model, $\mathcal{F}$, given the same parameters, $\Theta$.
These observations are chosen such that $X_i, X_p \in \Gamma_k$ and $X_n \in \bar{\Gamma_k}$.
Euclidean distance is computed between the positive and negative embedding pairs, and two margins, $m_p$ and $m_n$, define a floor on the positive and negative loss terms.

This loss is computed over a mini-batch of observations, differentiated with respect to the parameters of model, and back-propagated through the network via simple stochastic gradient descent.
Mini-batches consist of 32 triples, and training proceeds for 50k iterations.

\subsection{Data}

% The data source used herein is drawn from the Vienna Symphonic Library (VSL), a massive collection of studio-grade orchestral instrument samples recorded over a variety of performance techniques\footnote{\url{https://vsl.co.at/en}}.
We use a previously compiled collection of solo instrument samples, comprised of 5k instances drawn from 24 instrument classes \cite{humphrey2015dl4mir}.
The set is partitioned into 72k, 18k, and 30k for training, validation, and testing, respectively.
The crux of this exploration lies in \emph{how} neighborhoods are defined and sampled for training, and thus various relationships are considered to encourage different kinds of embeddings, including instrument class, absolute pitch, instrument class and absolute pitch, and instrument class and absolute pitch $\pm~2$ semitones.
% \item Instrument class and pitch chroma


\section{Evaluation}

After fitting models to the different neighborhood relationships, we consider a few approaches to assessing the organization of the resulting embeddings.
% Neighbors
Quantitatively, $k$-nearest neighbor classification measures the extent to which neighborhoods are preserved, and brute-force distance is used to sort data in a ranked retrieval setting.
Overall we find the learned embeddings are well-organized and consistently demonstrate robust performance in query-by-example tasks.
% Consistent with previous results \cite{humphrey2011nlse, humphrey2015dl4mir}, the sharpness of class boundaries is inversely to the discrete number of neighborhoods, and is likely due in part to the limits of three-space.
% Analogy
Additionally, these representations lend themselves well to more qualitative inquiries.
Visualization is the most straightforward way of exploring the learned embeddings, revealing some surprising behavior, as in Figure \ref{fig:asdr}.
Acoustic ``analogy'' serves as another good, if subjective, test of semantic organization, where the resultant vector between two points is applied to a third.
Lastly, trajectories in space are sonified via concatenative synthesis, providing audible insight into the learned embeddings.



\begin{figure}[!t]
\vskip -0.2in
\begin{center}
\centerline{\includegraphics[width=3.2in]{inst-pitch-d2_adsr.pdf}}
\vskip -0.2in
\caption{Embedding space revealing an ADSR envelope.}
\label{fig:asdr}
\end{center}
% \vskip -0.2in
\end{figure}



\section{Summary}

We have explored a range of embeddings learned using a convolutional neural network optimized to preserve various neighborhood relationships between instrument sounds in 3 dimensions.
We have explored a range of embeddings learned using a convolutional neural network optimized to preserve various neighborhood relationships between instrument sounds in 3 dimensions.
We have explored a range of embeddings learned using a convolutional neural network optimized to preserve various neighborhood relationships between instrument sounds in 3 dimensions.
% We have explored a range of embeddings learned using a convolutional neural network optimized to preserve various neighborhood relationships between instrument sounds in 3 dimensions.
% We have explored a range of embeddings learned using a convolutional neural network optimized to preserve various neighborhood relationships between instrument sounds in 3 dimensions.

% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% probably should) include acknowledgements. In this case, please
% place such acknowledgements in an unnumbered section at the
% end of the paper. Typically, this will include thanks to reviewers
% who gave useful comments, to colleagues who contributed to the ideas,
% and to funding agencies and corporate sponsors that provided financial
% support.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2016}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz,
% slightly modified from the 2009 version by Kiri Wagstaff and
% Sam Roweis's 2008 version, which is slightly modified from
% Prasad Tadepalli's 2007 version which is a lightly
% changed version of the previous year's version by Andrew Moore,
% which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
